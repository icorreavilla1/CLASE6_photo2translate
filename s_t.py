import os
import streamlit as st
from bokeh.models.widgets import Button
from bokeh.models import CustomJS
from streamlit_bokeh_events import streamlit_bokeh_events
from PIL import Image
import time
import glob
import cv2
import numpy as np
import pytesseract
from gtts import gTTS
from googletrans import Translator

st.title("Traductor de im√°genes")
st.subheader("Reconocimiento √≥ptico de im√°genes y traducci√≥n de texto")

translator = Translator()  # Inicializar el traductor

# Mostrar una imagen de ejemplo en la interfaz
image = Image.open('traductorimg.png')
st.image(image, width=300)

with st.sidebar:
    st.subheader("Traductor.")
    st.write("Presiona el bot√≥n, cuando escuches la se√±al "
             "habla lo que quieres traducir, luego selecciona"
             " la configuraci√≥n de lenguaje que necesites.")

st.write("Toca el Bot√≥n y habla lo que quieres traducir")

stt_button = Button(label=" Escuchar  üé§", width=300, height=50)

stt_button.js_on_event("button_click", CustomJS(code="""
    var recognition = new webkitSpeechRecognition();
    recognition.continuous = true;
    recognition.interimResults = true;

    recognition.onresult = function (e) {
        var value = "";
        for (var i = e.resultIndex; i < e.results.length; ++i) {
            if (e.results[i].isFinal) {
                value += e.results[i][0].transcript;
            }
        }
        if (value != "") {
            document.dispatchEvent(new CustomEvent("GET_TEXT", {detail: value}));
        }
    }
    recognition.start();
    """))

result = streamlit_bokeh_events(
    stt_button,
    events="GET_TEXT",
    key="listen",
    refresh_on_update=False,
    override_height=75,
    debounce_time=0
)

img_file_buffer = st.camera_input("Toma una Foto")

if img_file_buffer is not None:
    # Leer la imagen desde el buffer
    bytes_data = img_file_buffer.getvalue()
    cv2_img = cv2.imdecode(np.frombuffer(bytes_data, np.uint8), cv2.IMREAD_COLOR)

    # Convertir la imagen a RGB y extraer el texto
    img_rgb = cv2.cvtColor(cv2_img, cv2.COLOR_BGR2RGB)
    text = pytesseract.image_to_string(img_rgb)
    st.write("Texto detectado:")
    st.write(text)

    # Selecci√≥n de idioma de entrada
    in_lang = st.selectbox(
        "Selecciona el lenguaje de Entrada",
        ("Ingl√©s", "Espa√±ol", "Bengali", "Coreano", "Mandar√≠n", "Japon√©s")
    )
    if in_lang == "Ingl√©s":
        input_language = "en"
    elif in_lang == "Espa√±ol":
        input_language = "es"
    elif in_lang == "Bengali":
        input_language = "bn"
    elif in_lang == "Coreano":
        input_language = "ko"
    elif in_lang == "Mandar√≠n":
        input_language = "zh-cn"
    elif in_lang == "Japon√©s":
        input_language = "ja"

    # Selecci√≥n de idioma de salida
    out_lang = st.selectbox(
        "Selecciona el lenguaje de salida",
        ("Ingl√©s", "Espa√±ol", "Bengali", "Coreano", "Mandar√≠n", "Japon√©s")
    )
    if out_lang == "Ingl√©s":
        output_language = "en"
    elif out_lang == "Espa√±ol":
        output_language = "es"
    elif out_lang == "Bengali":
        output_language = "bn"
    elif out_lang == "Coreano":
        output_language = "ko"
    elif out_lang == "Mandar√≠n":
        output_language = "zh-cn"
    elif out_lang == "Japon√©s":
        output_language = "ja"

    def text_to_speech(input_language, output_language, text, tld='com'):
        translation = translator.translate(text, src=input_language, dest=output_language)
        trans_text = translation.text
        tts = gTTS(trans_text, lang=output_language, tld=tld, slow=False)
        my_file_name = text[0:20].replace(" ", "_")
        tts.save(f"temp/{my_file_name}.mp3")
        return my_file_name, trans_text

    display_output_text = st.checkbox("Mostrar el texto traducido")

    if st.button("Convertir"):
        try:
            result, output_text = text_to_speech(input_language, output_language, text)
            audio_file = open(f"temp/{result}.mp3", "rb")
            audio_bytes = audio_file.read()
            st.markdown("## Tu audio:")
            st.audio(audio_bytes, format="audio/mp3", start_time=0)

            if display_output_text:
                st.markdown("## Texto traducido:")
                st.write(output_text)
        except Exception as e:
            st.error(f"Error al traducir o convertir el texto: {e}")

    def remove_files(n):
        mp3_files = glob.glob("temp/*.mp3")
        if len(mp3_files) != 0:
            now = time.time()
            n_days = n * 86400
            for f in mp3_files:
                if os.stat(f).st_mtime < now - n_days:
                    os.remove(f)
                    print("Deleted ", f)

    remove_files(7)


           


        
    






    


